{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://cocl.us/pytorch_link_top\">\n",
    "    <img src=\"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0110EN/notebook_images%20/Pytochtop.png\" width=\"750\" alt=\"IBM Product \">\n",
    "</a> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-DL0321EN-SkillsNetwork/image/IDSN-logo.png\" width=\"200\" alt=\"cognitiveclass.ai logo\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><h1>Pre-trained-Models with PyTorch </h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, you will use pre-trained models to classify between the negative and positive samples; you will be provided with the dataset object. The particular pre-trained model will be resnet18; you will have three questions: \n",
    "<ul>\n",
    "<li>change the output layer</li>\n",
    "<li> train the model</li> \n",
    "<li>  identify  several  misclassified samples</li> \n",
    " </ul>\n",
    "You will take several screenshots of your work and share your notebook. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Table of Contents</h2>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">\n",
    "\n",
    "\n",
    "<ul>\n",
    "    <li><a href=\"#download_data\"> Download Data</a></li>\n",
    "    <li><a href=\"#auxiliary\"> Imports and Auxiliary Functions </a></li>\n",
    "    <li><a href=\"#data_class\"> Dataset Class</a></li>\n",
    "    <li><a href=\"#Question_1\">Question 1</a></li>\n",
    "    <li><a href=\"#Question_2\">Question 2</a></li>\n",
    "    <li><a href=\"#Question_3\">Question 3</a></li>\n",
    "</ul>\n",
    "<p>Estimated Time Needed: <strong>120 min</strong></p>\n",
    " </div>\n",
    "<hr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"download_data\">Download Data</h2>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the dataset and unzip the files in your data directory, unlike the other labs, all the data will be deleted after you close  the lab, this may take some time:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!wget https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0321EN/data/images/Positive_tensors.zip "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!unzip -qn Positive_tensors.zip "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!wget https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0321EN/data/images/Negative_tensors.zip\n",
    "#!unzip -qn Negative_tensors.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will install torchvision:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"auxiliary\">Imports and Auxiliary Functions</h2>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following are the libraries we are going to use for this lab. The <code>torch.manual_seed()</code> is for forcing the random function to give the same number every time we try to recompile it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f3304021e10>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# These are the libraries will be used for this lab.\n",
    "import torchvision.models as models\n",
    "from PIL import Image\n",
    "import pandas\n",
    "from torchvision import transforms\n",
    "import torch.nn as nn\n",
    "import time\n",
    "import torch \n",
    "import matplotlib.pylab as plt\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import h5py\n",
    "import os\n",
    "import glob\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from matplotlib.pyplot import imshow\n",
    "import matplotlib.pylab as plt\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--Empty Space for separating topics-->\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000\n",
      "20000\n"
     ]
    }
   ],
   "source": [
    "#Count the number of files unzipped\n",
    "directory=\"./\"\n",
    "positive=\"Positive_tensors\"\n",
    "negative='Negative_tensors'\n",
    "positive_file_path=os.path.join(directory,positive)\n",
    "negative_file_path=os.path.join(directory,negative)\n",
    "positive_files=[os.path.join(positive_file_path,file) for file in os.listdir(positive_file_path) if file.endswith(\".pt\")]\n",
    "negative_files=[os.path.join(negative_file_path,file) for file in os.listdir(negative_file_path) if file.endswith(\".pt\")]\n",
    "number_of_samples=len(positive_files)+len(negative_files)\n",
    "print(len(positive_files))\n",
    "print(len(negative_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"data_class\">Dataset Class</h2>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " This dataset class is essentially the same dataset you build in the previous section, but to speed things up, we are going to use tensors instead of jpeg images. Therefor for each iteration, you will skip the reshape step, conversion step to tensors and normalization step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "# Create your own dataset object\n",
    "\n",
    "class Dataset(Dataset):\n",
    "\n",
    "    # Constructor\n",
    "    def __init__(self, transform=None, train=True):\n",
    "        directory = \"./\"\n",
    "        positive = \"Positive_tensors\"\n",
    "        negative = 'Negative_tensors'\n",
    "\n",
    "        positive_file_path = os.path.join(directory, positive)\n",
    "        negative_file_path = os.path.join(directory, negative)\n",
    "\n",
    "        # Initialize lists to hold valid files and their corresponding labels\n",
    "        self.all_files = []\n",
    "        self.Y = []\n",
    "\n",
    "        # Function to check if a file can be loaded\n",
    "        def is_loadable(file_path):\n",
    "            return True    # assume all files can be loaded\n",
    "#            try:\n",
    "#                _ = torch.load(file_path)\n",
    "#                return True\n",
    "#            except Exception as e:\n",
    "#                print(f\"Error loading {file_path}: {e}. Skipping this file.\")\n",
    "#                return False\n",
    "\n",
    "        # Load positive files\n",
    "        positive_files = [os.path.join(positive_file_path, file) for file in os.listdir(positive_file_path) if file.endswith(\".pt\")]\n",
    "        count = 0\n",
    "        for file in positive_files:\n",
    "            if count % 500 == 0 and count >= 500:\n",
    "                print(f\"{count} positive files read.\")\n",
    "            if is_loadable(file):\n",
    "                self.all_files.append(file)\n",
    "                self.Y.append(1)  # Label for positive files\n",
    "            count += 1\n",
    "            \n",
    "        # Load negative files\n",
    "        negative_files = [os.path.join(negative_file_path, file) for file in os.listdir(negative_file_path) if file.endswith(\".pt\")]\n",
    "        count = 0\n",
    "        for file in negative_files:\n",
    "            if count % 500 == 0 and count >= 500:\n",
    "                print(f\"{count} negative files read.\")\n",
    "            if is_loadable(file):\n",
    "                self.all_files.append(file)\n",
    "                self.Y.append(0)  # Label for negative files\n",
    "            count += 1\n",
    "                \n",
    "        # Convert the labels list to a LongTensor\n",
    "        self.Y = torch.tensor(self.Y, dtype=torch.long)\n",
    "\n",
    "        # Apply transform\n",
    "        self.transform = transform\n",
    "        \n",
    "        # Subset the dataset for training or validation\n",
    "        if train:\n",
    "            self.all_files = self.all_files[:30000]\n",
    "            self.Y = self.Y[:30000]\n",
    "        else:\n",
    "            self.all_files = self.all_files[30000:]\n",
    "            self.Y = self.Y[30000:]\n",
    "\n",
    "        # Set the length of the dataset\n",
    "        self.len = len(self.all_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = torch.load(self.all_files[idx])\n",
    "        y = self.Y[idx]\n",
    "\n",
    "        # If there is any transform method, apply it onto the image\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "                \n",
    "        return image, y\n",
    "\n",
    "    def __len__(self):\n",
    "        # Return the length of the dataset\n",
    "        return len(self.all_files)\n",
    "    \n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create two dataset objects, one for the training data and one for the validation data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500 positive files read.\n",
      "1000 positive files read.\n",
      "1500 positive files read.\n",
      "2000 positive files read.\n",
      "2500 positive files read.\n",
      "3000 positive files read.\n",
      "3500 positive files read.\n",
      "4000 positive files read.\n",
      "4500 positive files read.\n",
      "5000 positive files read.\n",
      "5500 positive files read.\n",
      "6000 positive files read.\n",
      "6500 positive files read.\n",
      "7000 positive files read.\n",
      "7500 positive files read.\n",
      "8000 positive files read.\n",
      "8500 positive files read.\n",
      "9000 positive files read.\n",
      "9500 positive files read.\n",
      "10000 positive files read.\n",
      "10500 positive files read.\n",
      "11000 positive files read.\n",
      "11500 positive files read.\n",
      "12000 positive files read.\n",
      "12500 positive files read.\n",
      "13000 positive files read.\n",
      "13500 positive files read.\n",
      "14000 positive files read.\n",
      "14500 positive files read.\n",
      "15000 positive files read.\n",
      "15500 positive files read.\n",
      "16000 positive files read.\n",
      "16500 positive files read.\n",
      "17000 positive files read.\n",
      "17500 positive files read.\n",
      "18000 positive files read.\n",
      "18500 positive files read.\n",
      "19000 positive files read.\n",
      "19500 positive files read.\n",
      "500 negative files read.\n",
      "1000 negative files read.\n",
      "1500 negative files read.\n",
      "2000 negative files read.\n",
      "2500 negative files read.\n",
      "3000 negative files read.\n",
      "3500 negative files read.\n",
      "4000 negative files read.\n",
      "4500 negative files read.\n",
      "5000 negative files read.\n",
      "5500 negative files read.\n",
      "6000 negative files read.\n",
      "6500 negative files read.\n",
      "7000 negative files read.\n",
      "7500 negative files read.\n",
      "8000 negative files read.\n",
      "8500 negative files read.\n",
      "9000 negative files read.\n",
      "9500 negative files read.\n",
      "10000 negative files read.\n",
      "10500 negative files read.\n",
      "11000 negative files read.\n",
      "11500 negative files read.\n",
      "12000 negative files read.\n",
      "12500 negative files read.\n",
      "13000 negative files read.\n",
      "13500 negative files read.\n",
      "14000 negative files read.\n",
      "14500 negative files read.\n",
      "15000 negative files read.\n",
      "15500 negative files read.\n",
      "16000 negative files read.\n",
      "16500 negative files read.\n",
      "17000 negative files read.\n",
      "17500 negative files read.\n",
      "18000 negative files read.\n",
      "18500 negative files read.\n",
      "19000 negative files read.\n",
      "19500 negative files read.\n",
      "500 positive files read.\n",
      "1000 positive files read.\n",
      "1500 positive files read.\n",
      "2000 positive files read.\n",
      "2500 positive files read.\n",
      "3000 positive files read.\n",
      "3500 positive files read.\n",
      "4000 positive files read.\n",
      "4500 positive files read.\n",
      "5000 positive files read.\n",
      "5500 positive files read.\n",
      "6000 positive files read.\n",
      "6500 positive files read.\n",
      "7000 positive files read.\n",
      "7500 positive files read.\n",
      "8000 positive files read.\n",
      "8500 positive files read.\n",
      "9000 positive files read.\n",
      "9500 positive files read.\n",
      "10000 positive files read.\n",
      "10500 positive files read.\n",
      "11000 positive files read.\n",
      "11500 positive files read.\n",
      "12000 positive files read.\n",
      "12500 positive files read.\n",
      "13000 positive files read.\n",
      "13500 positive files read.\n",
      "14000 positive files read.\n",
      "14500 positive files read.\n",
      "15000 positive files read.\n",
      "15500 positive files read.\n",
      "16000 positive files read.\n",
      "16500 positive files read.\n",
      "17000 positive files read.\n",
      "17500 positive files read.\n",
      "18000 positive files read.\n",
      "18500 positive files read.\n",
      "19000 positive files read.\n",
      "19500 positive files read.\n",
      "500 negative files read.\n",
      "1000 negative files read.\n",
      "1500 negative files read.\n",
      "2000 negative files read.\n",
      "2500 negative files read.\n",
      "3000 negative files read.\n",
      "3500 negative files read.\n",
      "4000 negative files read.\n",
      "4500 negative files read.\n",
      "5000 negative files read.\n",
      "5500 negative files read.\n",
      "6000 negative files read.\n",
      "6500 negative files read.\n",
      "7000 negative files read.\n",
      "7500 negative files read.\n",
      "8000 negative files read.\n",
      "8500 negative files read.\n",
      "9000 negative files read.\n",
      "9500 negative files read.\n",
      "10000 negative files read.\n",
      "10500 negative files read.\n",
      "11000 negative files read.\n",
      "11500 negative files read.\n",
      "12000 negative files read.\n",
      "12500 negative files read.\n",
      "13000 negative files read.\n",
      "13500 negative files read.\n",
      "14000 negative files read.\n",
      "14500 negative files read.\n",
      "15000 negative files read.\n",
      "15500 negative files read.\n",
      "16000 negative files read.\n",
      "16500 negative files read.\n",
      "17000 negative files read.\n",
      "17500 negative files read.\n",
      "18000 negative files read.\n",
      "18500 negative files read.\n",
      "19000 negative files read.\n",
      "19500 negative files read.\n",
      "30000\n",
      "10000\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "train_dataset = Dataset(train=True)\n",
    "validation_dataset = Dataset(train=False)\n",
    "print(len(train_dataset))\n",
    "print(len(validation_dataset))\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"Question_1\">Question 1</h2>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Prepare a pre-trained resnet18 model :</b>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Step 1</b>: Load the pre-trained model <code>resnet18</code> Set the parameter <code>pretrained</code> to true:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/torchvision/models/_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and may be removed in the future, \"\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /home/jupyterlab/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5118d84aa8a74236a15cdae1c7ffb40b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0.00/44.7M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Step 1: Load the pre-trained model resnet18\n",
    "\n",
    "# Type your code here\n",
    "model = models.resnet18(pretrained=True)\n",
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]\n",
    "\n",
    "#composed=transforms.Compose[transforms.Resize(224),transforms.ToTensor,transforms.Normalize(mean, std)])\n",
    "#train_dataset=Dataset(transform=composed, train=True )\n",
    "#validation_data=Dataset(transform=composed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Step 2</b>: Set the attribute <code>requires_grad</code> to <code>False</code>. As a result, the parameters will not be affected by training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Set the parameter cannot be trained for the pre-trained model\n",
    "\n",
    "# Type your code here\n",
    "for param in model.parameters():\n",
    "    param.requires_grad=False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code>resnet18</code> is used to classify 1000 different objects; as a result, the last layer has 1000 outputs.  The 512 inputs come from the fact that the previously hidden layer has 512 outputs. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Step 3</b>: Replace the output layer <code>model.fc</code> of the neural network with a <code>nn.Linear</code> object, to classify 2 different classes. For the parameters <code>in_features </code> remember the last hidden layer has 512 neurons.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fc=nn.Linear(512,7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print out the model in order to show whether you get the correct answer.<br> <b>(Your peer reviewer is going to mark based on what you print here.)</b>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=512, out_features=7, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"Question_2\">Question 2: Train the Model</h2>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this question you will train your, model:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Step 1</b>: Create a cross entropy criterion function \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Create the loss function\n",
    "\n",
    "# Type your code here\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Step 2</b>: Create a training loader and validation loader object, the batch size should have 100 samples each.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training batches =  300\n",
      "Number of validation batches =  100\n"
     ]
    }
   ],
   "source": [
    "train_loader=torch.utils.data.DataLoader(dataset=train_dataset, batch_size=100)\n",
    "validation_loader=torch.utils.data.DataLoader(dataset=validation_dataset,batch_size=100)\n",
    "print(\"Number of training batches = \", len(train_loader))\n",
    "print(\"Number of validation batches = \", len(validation_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Step 3</b>: Use the following optimizer to minimize the loss \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam([parameters  for parameters in model.parameters() if parameters.requires_grad],lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--Empty Space for separating topics-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Complete the following code to calculate  the accuracy on the validation data for one epoch; this should take about 45 minutes. Make sure you calculate the accuracy on the validation data.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on training batch 0 to 9...\n",
      "Working on training batch 10 to 19...\n",
      "Working on training batch 20 to 29...\n",
      "Working on training batch 30 to 39...\n",
      "Working on training batch 40 to 49...\n",
      "Working on training batch 50 to 59...\n",
      "Working on training batch 60 to 69...\n",
      "Working on training batch 70 to 79...\n",
      "Working on training batch 80 to 89...\n",
      "Working on training batch 90 to 99...\n",
      "Working on training batch 100 to 109...\n",
      "Working on training batch 110 to 119...\n",
      "Working on training batch 120 to 129...\n",
      "Working on training batch 130 to 139...\n",
      "Working on training batch 140 to 149...\n",
      "Working on training batch 150 to 159...\n",
      "Working on training batch 160 to 169...\n",
      "Working on training batch 170 to 179...\n",
      "Working on training batch 180 to 189...\n",
      "Working on training batch 190 to 199...\n",
      "Working on training batch 200 to 209...\n",
      "Working on training batch 210 to 219...\n",
      "Working on training batch 220 to 229...\n",
      "Working on training batch 230 to 239...\n",
      "Working on training batch 240 to 249...\n",
      "Working on training batch 250 to 259...\n",
      "Working on training batch 260 to 269...\n",
      "Working on training batch 270 to 279...\n",
      "Working on training batch 280 to 289...\n",
      "Working on training batch 290 to 299...\n",
      "Working on validation batch 0 to 9...\n",
      "Working on validation batch 10 to 19...\n",
      "Working on validation batch 20 to 29...\n",
      "Working on validation batch 30 to 39...\n",
      "Working on validation batch 40 to 49...\n",
      "Working on validation batch 50 to 59...\n",
      "Working on validation batch 60 to 69...\n",
      "Working on validation batch 70 to 79...\n",
      "Working on validation batch 80 to 89...\n",
      "Working on validation batch 90 to 99...\n"
     ]
    }
   ],
   "source": [
    "n_epochs=1\n",
    "loss_list=[]\n",
    "accuracy_list=[]\n",
    "correct=0\n",
    "N_test=len(validation_dataset)\n",
    "N_train=len(train_dataset)\n",
    "start_time = time.time()\n",
    "#n_epochs\n",
    "\n",
    "Loss=0\n",
    "start_time = time.time()\n",
    "for epoch in range(n_epochs):\n",
    "    batchnum = 0\n",
    "    for x, y in train_loader:\n",
    "        if x is None or y is None:\n",
    "            continue  # Skip this batch\n",
    "        if batchnum % 10 == 0:\n",
    "            print(f\"Working on training batch {batchnum} to {batchnum+9}...\")\n",
    "        model.train() \n",
    "        #clear gradient \n",
    "        optimizer.zero_grad()\n",
    "        #make a prediction \n",
    "        z = model(x)\n",
    "        # calculate loss \n",
    "        loss = criterion(z, y)\n",
    "        # calculate gradients of parameters\n",
    "        loss.backward()\n",
    "        # update parameters \n",
    "        optimizer.step()\n",
    "        loss_list.append(loss.data)\n",
    "        batchnum += 1\n",
    "\n",
    "    correct=0\n",
    "    batchnum=0\n",
    "    for x_test, y_test in validation_loader:\n",
    "        if x_test is None or y_test is None:\n",
    "            continue  # Skip this batch\n",
    "        if batchnum % 10 == 0:\n",
    "            print(f\"Working on validation batch {batchnum} to {batchnum+9}...\")\n",
    "        # set model to eval \n",
    "        model.eval()\n",
    "        #make a prediction \n",
    "        z = model(x_test)\n",
    "        #find max \n",
    "        _, yhat = torch.max(z.data, 1)    \n",
    "        #Calculate misclassified  samples in mini-batch \n",
    "        #hint +=(yhat==y_test).sum().item()\n",
    "        correct += (yhat == y_test).sum().item()\n",
    "        batchnum += 1\n",
    "   \n",
    "    accuracy=correct/N_test\n",
    "    accuracy_list.append(accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Print out the Accuracy and plot the loss stored in the list <code>loss_list</code> for every iteration and take a screen shot.</b>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss_list)\n",
    "plt.xlabel(\"iteration\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"Question_3\">Question 3:Find the misclassified samples</h2> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Identify the first four misclassified samples using the validation data:</b>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#define a function to display the tensor image\n",
    "def display_tensor_image(tensor_img):\n",
    "    if tensor_img.is_cuda:\n",
    "        tensor_img = tensor_img.cpu()\n",
    "    \n",
    "    if tensor_img.dim() == 4:\n",
    "        tensor_img = tensor_img.squeeze(0)\n",
    "\n",
    "    if tensor_img.shape[0] == 3:\n",
    "        tensor_img = tensor_img.permute(1, 2, 0)\n",
    "    \n",
    "    img_np = tensor_img.numpy()\n",
    "    img_np = (img_np * 0.5) + 0.5\n",
    "    img_np = np.clip(img_np, 0, 1)\n",
    "\n",
    "    plt.imshow(img_np)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 1\n",
    "for x_test, y_test in validation_loader:\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        z = model(x_test)\n",
    "        _, yhat = torch.max(z.data, 1)\n",
    "        for j in range(len(y_test)):\n",
    "            if yhat[j] != y_test[j]:\n",
    "                print(f\"Misclassified {i}: predicted is {yhat[j].item()}, actual is {y_test[j].item()}\")\n",
    "                display_tensor_image(x_test[j])\n",
    "                i += 1\n",
    "                # max four misclassified samples\n",
    "                if i > 4:\n",
    "                    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://dataplatform.cloud.ibm.com/docs/content/wsj/analyze-data/share-notebooks.html?utm_source=Exinfluencer&utm_content=000026UJ&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0321ENSkillsNetwork951-2022-01-01&utm_medium=Exinfluencer&utm_term=10006555\"> CLICK HERE </a> Click here to see how to share your notebook.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>About the Authors:</h2> \n",
    "\n",
    "<a href=\"https://www.linkedin.com/in/joseph-s-50398b136/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0321ENSkillsNetwork951-2022-01-01\">Joseph Santarcangelo</a> has a PhD in Electrical Engineering, his research focused on using machine learning, signal processing, and computer vision to determine how videos impact human cognition. Joseph has been working for IBM since he completed his PhD.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Change Log\n",
    "\n",
    "|  Date (YYYY-MM-DD) |  Version | Changed By  |  Change Description |\n",
    "|---|---|---|---|\n",
    "| 2020-09-21  | 2.0  | Shubham  |  Migrated Lab to Markdown and added to course repo in GitLab |\n",
    "\n",
    "\n",
    "\n",
    "<hr>\n",
    "\n",
    "## <h3 align=\"center\">  IBM Corporation 2020. All rights reserved. <h3/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright &copy; 2018 <a href=\"cognitiveclass.ai?utm_source=bducopyrightlink&utm_medium=dswb&utm_campaign=bdu\">cognitiveclass.ai</a>. This notebook and its source code are released under the terms of the <a href=\"https://bigdatauniversity.com/mit-license/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0321ENSkillsNetwork951-2022-01-01\">MIT License</a>.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "conda-env-python-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
